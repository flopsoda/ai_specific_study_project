import os
from typing import List, TypedDict, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
import asyncio
from config import CHARACTERS, MAIN_WRITER_CONFIG, CHARACTER_AGENT_CONFIG
from shared import global_state
from langgraph.graph import END
from utils import get_story_context
from memory import lore_book # [ì¶”ê°€]

# ---ê·¸ë˜í”„ì˜ ìƒíƒœ(State) ì •ì˜---
class GraphState(TypedDict):
    story_parts: List[str]
    current_context: str
    retrieved_memory: str   # [ì¶”ê°€] ì´ë²ˆ í„´ì— ì‚¬ìš©í•  ê³¼ê±° ê¸°ì–µ (RAG ê²°ê³¼)
    discussion : list[str]
    selected_character: str
    user_decision: Optional[str]

# [ì¶”ê°€] ë©”ì¸ ì‘ê°€ìš© LLM ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
WRITER_LLM = ChatGoogleGenerativeAI(
    model=MAIN_WRITER_CONFIG["model"],
    temperature=MAIN_WRITER_CONFIG["temperature"]
)

# ---í† ë¡  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì´ì•¼ê¸°ë¥¼ ì‘ì„±í•˜ëŠ” ë©”ì¸ ì‘ê°€ ì—ì´ì „íŠ¸---
def main_writer_node(state: GraphState) -> dict:
    """
    ì§€ê¸ˆê¹Œì§€ì˜ ì´ì•¼ê¸°ì™€ ìºë¦­í„°ë“¤ì˜ í† ë¡  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ ì´ì•¼ê¸° ë‹¨ë½ì„ ì‘ì„±í•©ë‹ˆë‹¤.
    """
    global_state["current_status"] = "âœï¸ ë©”ì¸ ì‘ê°€ê°€ ì´ì•¼ê¸°ë¥¼ ì§‘í•„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
    
    print("\n--- ë©”ì¸ ì‘ê°€ ì—ì´ì „íŠ¸ ì‘ë™ ---")
    
    # ì‘ê°€ëŠ” í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ê¸€ì„ ì”ë‹ˆë‹¤.
    story_so_far = state.get("current_context", "")
    context = state.get("retrieved_memory", "") # [ì¶”ê°€] ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°

    discussion_str = "\n".join(state["discussion"])
    
    prompt = MAIN_WRITER_CONFIG["prompt_template"].format(
        world_name=MAIN_WRITER_CONFIG["world_name"],
        world_description=MAIN_WRITER_CONFIG["world_description"],
        context=context, # [ìˆ˜ì •] Stateì—ì„œ ë°›ì€ ë©”ëª¨ë¦¬ ì£¼ì…
        story_so_far=story_so_far,
        discussion_str=discussion_str
    )
    response = WRITER_LLM.invoke(prompt)
    next_part = response.content.strip()
    print(f"\n[ë©”ì¸ ì‘ê°€] ì´ì•¼ê¸° ìƒì„± ì™„ë£Œ:\n{next_part[:100]}...\n")
    
    # [í•µì‹¬ ë³€ê²½] ì´ì•¼ê¸°ê°€ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ, ë‹¤ìŒ í„´ì„ ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¯¸ë¦¬ ê°±ì‹ í•©ë‹ˆë‹¤.
    new_story_parts = state["story_parts"] + ["\n\n" + next_part]
    new_context = get_story_context(new_story_parts) # ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ê³„ì‚°!

    # [ì‚­ì œ] ì—¬ê¸°ì„œ ê²€ìƒ‰í•˜ë˜ ë¡œì§ ì œê±° (retrieve_memory_nodeë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ)
    # relevant_memory = lore_book.search_relevant_info(story_so_far) 
    
    # [ìœ ì§€] ê¸€ ë‹¤ ì“°ê³  ë‚˜ì„œ ì €ì¥(Archiving)í•˜ëŠ” ê±´ ì—¬ì „íˆ ì—¬ê¸°ì„œ í•´ì•¼ í•¨
    lore_book.check_and_archive(new_story_parts)
    
    return {
        "story_parts": new_story_parts,
        "current_context": new_context,
        # "retrieved_memory": "" # (ì„ íƒ) ë‹¤ìŒ í„´ì„ ìœ„í•´ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”? êµ³ì´ ì•ˆ í•´ë„ ë®ì–´ì”Œì›Œì§
    }

# --- ë…¸ë“œ(Node)ë¡œ ì‚¬ìš©í•  í•¨ìˆ˜ ì •ì˜ ---

## ---ìºë¦­í„° ì¤‘ ëˆ„ê°€ í† ë¡  ì¤‘ ì˜ê²¬ì„ ì œì‹œí• ì§€ ê²½ìŸí•˜ëŠ” í•¨ìˆ˜---
VOTE_LLM = ChatGoogleGenerativeAI(
    model = CHARACTER_AGENT_CONFIG["vote_model"],
    temperature=CHARACTER_AGENT_CONFIG["vote_temperature"]
)

# [ì¶”ê°€] ì˜ê²¬ ìƒì„±ìš© LLM ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
OPINION_LLM = ChatGoogleGenerativeAI(
    model=CHARACTER_AGENT_CONFIG["opinion_model"],
    temperature=CHARACTER_AGENT_CONFIG["opinion_temperature"]
)

# --- 1. íˆ¬í‘œ í—¬í¼ í•¨ìˆ˜ ìˆ˜ì • ---
async def _get_character_vote(character_name:str, story_so_far:str, discussion: list[str], context: str) -> Optional[str]:
    """ë‹¨ì¼ ì„œë¸Œ ì—ì´ì „íŠ¸ì˜ íˆ¬í‘œë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì–»ëŠ” í—¬í¼ í•¨ìˆ˜"""
    discussion_str = "\n".join(discussion)
    character_config = CHARACTERS[character_name]
    character_prompt = character_config["prompt"]
    prompt = CHARACTER_AGENT_CONFIG["prompt_templates"]["vote"].format(
        character_name=character_name,
        character_prompt=character_prompt,
        context=context, # [ìˆ˜ì •] Stateì—ì„œ ë°›ì€ ë©”ëª¨ë¦¬ ì£¼ì…
        story_so_far=story_so_far,
        discussion_str=discussion_str
    )
    try:
        response = await VOTE_LLM.ainvoke(prompt)
        vote = response.content.strip() 
        if "ë„¤" in vote:
           # print(f"--- {character_name}ì˜ íˆ¬í‘œ: {vote} (ì„ íƒ!) ---")
            return character_name
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì–´ë–¤ ìºë¦­í„°ì—ì„œ ë¬¸ì œê°€ ìˆì—ˆëŠ”ì§€ ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
        print(f"--- {character_name} íˆ¬í‘œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
        return None
    return None

## ---ê²½ìŸì„ í†µí•´ í–‰ë™í•  ìºë¦­í„°ë¥¼ ì„ íƒí•˜ëŠ” í•¨ìˆ˜---
async def race_for_action(state: GraphState) -> dict:
    """
    ëª¨ë“  ìºë¦­í„°ì—ê²Œ ë™ì‹œì— ë¬¼ì–´ë³´ê³ , ê°€ì¥ ë¨¼ì € 'ë„¤'ë¼ê³  ë‹µí•˜ëŠ” ìºë¦­í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    """
    global_state["current_status"] = "ğŸ‘€ ëˆˆì¹˜ ê²Œì„ ì¤‘... (ëˆ„ê°€ ë°œì–¸í• ì§€ ê²½ìŸ ì¤‘)"
    
    # [ìˆ˜ì •] ë§¤ë²ˆ ê³„ì‚°í•˜ì§€ ì•Šê³ , Stateì— ì €ì¥ëœ ê°’ì„ ë°”ë¡œ ì‚¬ìš©
    story_so_far = state.get("current_context", "")
    context = state.get("retrieved_memory", "") # [ì¶”ê°€] ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
    
    discussion = state["discussion"]
    # [ê²€ì¦ìš© ë¡œê·¸] ì‹¤ì œë¡œ ë¹„ì›Œì¡ŒëŠ”ì§€ í„°ë¯¸ë„ì—ì„œ í™•ì¸
    print(f"\n[DEBUG] í˜„ì¬ í† ë¡  ë‚´ì—­ ê°œìˆ˜: {len(discussion)}ê°œ")
    if len(discussion) > 0:
        print(f"[DEBUG] ì”ì—¬ ë°ì´í„° í™•ì¸: {discussion[0][:30]}...")
    else:
        print("[DEBUG] í† ë¡  ë‚´ì—­ì´ ê¹¨ë—í•˜ê²Œ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
    characters = list(CHARACTERS.keys()) # ê²½ìŸì— ì°¸ì—¬í•  ìºë¦­í„° ëª©ë¡
    # _get_character_vote í˜¸ì¶œ ì‹œ context ì „ë‹¬
    tasks = [asyncio.create_task(_get_character_vote(name, story_so_far, discussion, context)) for name in characters] 
    winner = None
    # asyncio.as_completedëŠ” ì‘ì—…ì´ ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    for future in asyncio.as_completed(tasks):
        try:
            result = await future
            if result:  # 'ë„¤'ë¼ê³  ë‹µí•œ ì²« ë²ˆì§¸ ìŠ¹ìë¥¼ ì°¾ìœ¼ë©´
                winner = result
                break # ì¦‰ì‹œ ë£¨í”„ë¥¼ ì¤‘ë‹¨í•˜ê³  ë” ì´ìƒ ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
        except asyncio.CancelledError:
            pass # ì·¨ì†Œëœ ì‘ì—…ì€ ë¬´ì‹œí•©ë‹ˆë‹¤.        
    # ìŠ¹ìê°€ ê²°ì •ë˜ì—ˆìœ¼ë¯€ë¡œ, ì•„ì§ ì‹¤í–‰ ì¤‘ì¸ ë‚˜ë¨¸ì§€ ì‘ì—…ë“¤ì„ ëª¨ë‘ ì·¨ì†Œí•©ë‹ˆë‹¤.
    for task in tasks:
        if not task.done():
            task.cancel()      
    if not winner:
        print("--- í–‰ë™í•˜ë ¤ëŠ” ìºë¦­í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ---")
        return {"selected_character": "None"}
    return {"selected_character": winner}

## ---ì„ íƒëœ ìºë¦­í„°ê°€ í† ë¡ ì— ëŒ€í•œ ì˜ê²¬ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜---
def generate_character_opinion(state: GraphState) -> dict:
    """ì„ íƒëœ ìºë¦­í„°ê°€ í† ë¡ ì— ëŒ€í•œ ì˜ê²¬ì„ ìƒì„±í•˜ê³  discussion ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    character_name = state["selected_character"]
    
    global_state["current_status"] = f"ğŸ—£ï¸ '{character_name}' ì‘ê°€ê°€ ë°œì–¸ì„ ì •ë¦¬í•˜ëŠ” ì¤‘..."

    if not character_name or character_name == "None":
        return {}

    # [ìˆ˜ì •] ë§¤ë²ˆ ê³„ì‚°í•˜ì§€ ì•Šê³ , Stateì— ì €ì¥ëœ ê°’ì„ ë°”ë¡œ ì‚¬ìš©
    story_so_far = state.get("current_context", "")
    context = state.get("retrieved_memory", "") # [ì¶”ê°€] ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
    
    discussion = state["discussion"]
    discussion_str = "\n".join(discussion)
    # ìºë¦­í„° ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    character_config = CHARACTERS[character_name]
    
    # [ìˆ˜ì •] ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ OPINION_LLM ì‚¬ìš©
    prompt = CHARACTER_AGENT_CONFIG["prompt_templates"]["generate_opinion"].format(
        character_name=character_name,
        character_prompt=character_config["prompt"],
        context=context, # [ìˆ˜ì •] Stateì—ì„œ ë°›ì€ ë©”ëª¨ë¦¬ ì£¼ì…
        story_so_far=story_so_far,
        discussion_str=discussion_str
    )
    response = OPINION_LLM.invoke(prompt)
    opinion = f"[{character_name} íŒŒíŠ¸ ë‹´ë‹¹ ì‘ê°€]: {response.content.strip()}" 
    print(opinion)
    # ìƒì„±ëœ ì˜ê²¬ì„ discussion ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    return {"discussion": discussion + [opinion]}

# [ìˆ˜ì •ë¨] ì‚¬ìš©ì ì…ë ¥ì„ ë¹„ë™ê¸°ë¡œ ê¸°ë‹¤ë¦¬ëŠ” ë…¸ë“œ
async def check_continuation(state: GraphState):
    print("\nâ³ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ [ê³„ì†í•˜ê¸°] ë˜ëŠ” [ì¢…ë£Œ]ë¥¼ ì„ íƒí•˜ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
    
    global_state["current_status"] = "â³ ë‹¹ì‹ ì˜ ì„ íƒ(ë˜ëŠ” ì‹ ì˜ ê°œì…)ì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤."

    # 1. ì›¹ UIì— ë²„íŠ¼ì„ ë„ìš°ë¼ê³  ì‹ í˜¸ë¥¼ ë³´ëƒ„
    global_state["waiting_for_input"] = True
    global_state["user_decision"] = None 
    global_state["user_instruction"] = None # ì´ˆê¸°í™”

    # 2. ì›¹ì—ì„œ ë²„íŠ¼ì„ ëˆ„ë¥¼ ë•Œê¹Œì§€ ë¬´í•œ ëŒ€ê¸°
    while global_state["user_decision"] is None:
        await asyncio.sleep(0.5)

    # 3. ê²°ì •ì´ ë‚´ë ¤ì§€ë©´ ì‹ í˜¸ë¥¼ ë„ê³  ì§„í–‰
    decision = global_state["user_decision"]
    instruction = global_state.get("user_instruction", "") # ì‚¬ìš©ì ì…ë ¥ ê°€ì ¸ì˜¤ê¸°

    global_state["waiting_for_input"] = False
    
    print(f"âœ… ì‚¬ìš©ì ì„ íƒ í™•ì¸: {decision}")
    if instruction:
        print(f"âš¡ ì‹ ì˜ ê°œì…: {instruction}")
    
    if decision == "continue":
        # [í•µì‹¬] ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©ì´ ìˆë‹¤ë©´, í† ë¡  ë¡œê·¸ì˜ ì²« ë²ˆì§¸ í•­ëª©ìœ¼ë¡œ ê°•ì œ ì£¼ì…í•©ë‹ˆë‹¤.
        # ì´ë ‡ê²Œ í•˜ë©´ ë‹¤ìŒ í„´ì˜ ìºë¦­í„°ë“¤ì´ ì´ ë‚´ìš©ì„ ë³´ê³  ê¸°ê²í•˜ë©° ë°˜ì‘í•˜ê²Œ ë©ë‹ˆë‹¤.
        new_discussion = []
        if instruction:
            system_msg = f"*** [ê¸´ê¸‰ ìƒí™© ë°œìƒ] ì™¸ë¶€ì˜ ì ˆëŒ€ì ì¸ í˜ì— ì˜í•´ ë‹¤ìŒ í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤: '{instruction}' ***\n(ëª¨ë“  ì‘ê°€ëŠ” ì´ ìƒí™©ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ ì „ê°œë¥¼ ë…¼ì˜í•˜ì‹­ì‹œì˜¤.)"
            new_discussion.append(system_msg)
            
        return {"user_decision": decision, "discussion": new_discussion} 
        
    return {"user_decision": decision}

# [ì¶”ê°€ë¨] ë¼ìš°íŒ… ë¡œì§
def route_continuation(state: GraphState):
    # check_continuation ë…¸ë“œì—ì„œ ê²°ì •ëœ ì‚¬í•­ì„ global_stateì—ì„œ í™•ì¸
    decision = global_state.get("user_decision")
    
    if decision == "continue":
        return "race_for_action"
    else:
        return END

# [ì‹ ê·œ] í† ë¡  ì‹œì‘ ì „, ê´€ë ¨ ê¸°ì–µì„ ê²€ìƒ‰í•˜ì—¬ Stateì— ì €ì¥í•˜ëŠ” ë…¸ë“œ
def retrieve_memory_node(state: GraphState) -> dict:
    print("\nğŸ§  [System] ì´ë²ˆ í„´ì— í•„ìš”í•œ ê³¼ê±° ê¸°ì–µì„ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ëŠ” í˜„ì¬ ì»¨í…ìŠ¤íŠ¸(ìµœê·¼ ì´ì•¼ê¸°)ë¥¼ ì‚¬ìš©
    query = state.get("current_context", "")
    if not query:
        query = get_story_context(state["story_parts"])
        
    # LoreBookì—ì„œ ê²€ìƒ‰
    memory = lore_book.search_relevant_info(query)
    
    if memory and "ì•„ì§ ê¸°ë¡ëœ" not in memory:
        print(f"ğŸ” ê²€ìƒ‰ëœ ê¸°ì–µ: {memory[:50]}...")
    else:
        print("ğŸ” ê²€ìƒ‰ëœ ê¸°ì–µ ì—†ìŒ (ì´ˆë°˜ì´ê±°ë‚˜ ë°ì´í„° ë¶€ì¡±)")
        
    return {"retrieved_memory": memory}








